{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ln52G_ho6vm"
      },
      "source": [
        "# Data preparation, exploration and sorting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bCE_pkoDm4s"
      },
      "source": [
        "# Downloading packages and loading libraries\n",
        "This section imports all the necessary libraries and modules required for the script to run. It includes libraries for data manipulation, visualization, machine learning models, image preprocessing, and model evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5mXaoaRpLbL"
      },
      "outputs": [],
      "source": [
        "! pip install tensorflow_addons\n",
        "! pip install visualkeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOyYHYpQ48rv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7TlsqXwQA_E"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import os\n",
        "import random\n",
        "from random import randint\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "from matplotlib import colormaps\n",
        "import visualkeras\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow.keras import Sequential, Input\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, Flatten, SeparableConv2D, BatchNormalization, MaxPool2D, GlobalAveragePooling2D, MaxPooling2D\n",
        "from tensorflow.keras.applications import VGG16, ResNet50, Xception, InceptionV3, DenseNet169, EfficientNetB0, MobileNet\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef as MCC, roc_curve, auc, balanced_accuracy_score as BAS, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from distutils.dir_util import copy_tree, remove_tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryZw7V1PFXbG"
      },
      "source": [
        "# Installing the Kaggle API client and setting up the Kaggle configuration file\n",
        "\n",
        "After running these commands, it is possible to use the Kaggle API client to interact with Kaggle datasets. `kaggle.json` file is created via Kaggle and uploaded manually to Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FYuvdTTZ9ta"
      },
      "outputs": [],
      "source": [
        "# Install the Kaggle API client package using pip\n",
        "! pip install kaggle\n",
        "# Create a directory named `.kaggle` in the user's home directory (`~`), which is where the Kaggle configuration file will be stored\n",
        "! mkdir ~/.kaggle\n",
        "# Copy the Kaggle API credentials file (`kaggle.json`) to the newly created `.kaggle` directory\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "# Change the permissions of the `kaggle.json` file to make it readable and writable only by the owner (the user), ensuring that the API credentials are secure\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Upw-2jxZEyPb"
      },
      "source": [
        "\n",
        "#Setting the random seed to the same value\n",
        "This ensures reproducibility of results when using random number generation functions, which is especially important in machine learning tasks where randomness is involved, such as weight initialization, data shuffling, or dropout. By setting the random seed, random numbers generated during different runs of the script remain the same, providing consistent and comparable results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1yHkjjmWqt2"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(5638)\n",
        "random.seed(5638)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-43RPKIGcCK"
      },
      "source": [
        "#Downloading the curated dataset from Kaggle\n",
        "\n",
        "This dataset is related to dementia and contains images categorized into three classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ne_ShfcqjTLw"
      },
      "outputs": [],
      "source": [
        "! kaggle datasets download -d zofiaknapiska/adni-selected-processed-brain-axial-t1-mri-jpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgEyMkn6Gqfe"
      },
      "source": [
        "#Unzipping and exploring the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2amyKPCk4Hd"
      },
      "outputs": [],
      "source": [
        "! unzip '/content/adni-selected-processed-brain-axial-t1-mri-jpeg.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xJCfXxVqsrT"
      },
      "source": [
        "#Preparing the data for training and testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlMPLVx-HBp5"
      },
      "source": [
        "#Defining parameters for training a neural network model\n",
        "\n",
        "Setting parameters and configurations for image processing and neural network training. These parameters help in standardizing the input images and controlling the training process, ensuring consistency and efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7Piv_bflG9k"
      },
      "outputs": [],
      "source": [
        "# Define the number of epochs, which is the number of times the entire dataset will be passed forward and backward through the neural network during training\n",
        "EPOCHS = 75\n",
        "\n",
        "# Set the batch size, which determines the number of samples that will be propagated through the network at a time\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Define the size to which the images will be resized in pixels\n",
        "IMG_SIZE = 192\n",
        "\n",
        "#Store the image size as a list with two elements, both set to 192\n",
        "IMAGE_SIZE = [192, 192, 3]\n",
        "\n",
        "# Define the dimensions to which the images will be resized as a tuple\n",
        "DIM = (IMG_SIZE, IMG_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kSY_OjRrCLt"
      },
      "source": [
        "# Visualizing the distribution of labels in the training set\n",
        "The labels correspond to the different types of dementia in the training dataset. They provide insight into the balance or imbalance of classes, which is crucial for training a machine learning model effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O0QI84Xbl9x3"
      },
      "outputs": [],
      "source": [
        "# Path to the directory containing training images\n",
        "train_dir = Path('/content/archive/train')\n",
        "\n",
        "# Get filepaths and labels\n",
        "filepaths = list(train_dir.glob(r'**/*.jpg'))\n",
        "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
        "\n",
        "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
        "labels = pd.Series(labels, name='Label')\n",
        "\n",
        "# Concatenate filepaths and labels\n",
        "train_df = pd.concat([filepaths, labels], axis=1)\n",
        "\n",
        "# Shuffle the DataFrame and reset index\n",
        "train_df = train_df.sample(frac=1).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBld5mi3h_ad"
      },
      "outputs": [],
      "source": [
        "# Split training data into train and validation sets\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['Label'], random_state=5638)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhs_yLFtrz35"
      },
      "source": [
        "# Visualizing the distribution of labels in the testing set\n",
        "The labels correspond to the different types of dementia in the testing dataset. They provide insight into the balance or imbalance of classes, which is crucial for testing a machine learning model effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzFRraOLmPl0"
      },
      "outputs": [],
      "source": [
        "# Path to the directory containing testing images\n",
        "test_dir = Path('/content/archive/test')\n",
        "\n",
        "# Get filepaths and labels\n",
        "filepaths = list(test_dir.glob(r'**/*.jpg'))\n",
        "labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n",
        "\n",
        "filepaths = pd.Series(filepaths, name='Filepath').astype(str)\n",
        "labels = pd.Series(labels, name='Label')\n",
        "\n",
        "# Concatenate filepaths and labels\n",
        "test_df = pd.concat([filepaths, labels], axis=1)\n",
        "\n",
        "# Shuffle the DataFrame and reset index\n",
        "test_df = test_df.sample(frac=1).reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSH8sGnAsi1M"
      },
      "source": [
        "#Utilizing `ImageDataGenerator` to prepare image data for training and testing\n",
        "Setting up data pipelines for training and testing deep learning models using Keras. Automating data preprocessing tasks such as scaling and batching, making it easier to work with large datasets efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtAs4_6Xmcxc"
      },
      "outputs": [],
      "source": [
        "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale=1.0/255.0,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.0/255.0)\n",
        "\n",
        "train_images = train_generator.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(192,192),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed = 5638,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_images = test_generator.flow_from_dataframe(\n",
        "    dataframe=val_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(192, 192),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed=5638,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_images = test_generator.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='Filepath',\n",
        "    y_col='Label',\n",
        "    target_size=(192, 192),\n",
        "    color_mode='rgb',\n",
        "    class_mode='categorical',\n",
        "    batch_size=BATCH_SIZE,\n",
        "    seed = 5638,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "CLASSES = list(test_images.class_indices.keys())\n",
        "num_classes = len(CLASSES)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZqmrZGTtzKL"
      },
      "source": [
        "#Calculating and visualizing the distribution of classes within both the training and testing datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aiQZhuhmqGg"
      },
      "outputs": [],
      "source": [
        "# Retrieve class indices mapping for both training and testing datasets\n",
        "train_class_indices = train_images.class_indices\n",
        "test_class_indices = test_images.class_indices\n",
        "\n",
        "# Get the total number of classes present in the dataset\n",
        "num_classes = len(train_class_indices)\n",
        "\n",
        "# Initialize dictionaries to store counts of each class in the training and testing datasets\n",
        "train_class_counts = {class_name: 0 for class_name in train_class_indices}\n",
        "test_class_counts = {class_name: 0 for class_name in test_class_indices}\n",
        "\n",
        "# Update counts for each class in the training set\n",
        "for label in train_images.labels:\n",
        "    class_name = CLASSES[label]\n",
        "    train_class_counts[class_name] += 1\n",
        "\n",
        "# Update counts for each class in the testing set\n",
        "for label in test_images.labels:\n",
        "    class_name = CLASSES[label]\n",
        "    test_class_counts[class_name] += 1\n",
        "\n",
        "# Combine counts for both training and testing datasets\n",
        "combined_class_counts = {}\n",
        "for class_name in train_class_counts.keys():\n",
        "    combined_class_counts[class_name] = [train_class_counts[class_name], test_class_counts[class_name]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCEXHw0Dw1Qf"
      },
      "source": [
        "#Working with the CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XwSKti4v_uG"
      },
      "source": [
        "#Defining a convolutional neural network (CNN) model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBdrIYTuiOKz"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Compute class weights\n",
        "class_weights = class_weight.compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.unique(train_df['Label']),\n",
        "    y=train_df['Label']\n",
        ")\n",
        "class_weights = {i: weight for i, weight in enumerate(class_weights)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBo5E0CNiSlS"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "    base_model = VGG16(weights='imagenet',\n",
        "                             include_top=False,\n",
        "                             input_shape=(192, 192, 3))\n",
        "\n",
        "    # Calculating the total number of layers in the base model\n",
        "    total_layers = len(base_model.layers)\n",
        "    # Calculating the number of layers to fine-tune (half of the total layers)\n",
        "    fine_tune_layers = total_layers // 2\n",
        "    # Unfreezing the top layers for fine-tuning\n",
        "    for layer in base_model.layers[:-fine_tune_layers]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Adding custom top layers for classification\n",
        "    x = base_model.output\n",
        "\n",
        "    # Adds additional feature extraction capabilities allowing the model to capture more task-specific patterns\n",
        "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
        "    # Normalizes the activations of the previous layer helping stabilize and speed up the training process\n",
        "    x = BatchNormalization()(x)\n",
        "    # Reduces the spatial dimensions of the feature maps helping focus on the most important features\n",
        "    x = MaxPooling2D((2, 2))(x)\n",
        "    # Prevents overfitting by randomly setting a fraction of weights to 0 during training\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Converts the 2D feature maps into a 1D feature vector\n",
        "    x = Flatten()(x)\n",
        "    # Introduces a fully connected layer allowing the model to learn complex combinations of the features extracted by the convolutional layers\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    # Further regularizes the model by preventing overfitting\n",
        "    x = Dropout(0.3)(x)\n",
        "\n",
        "    # Output layer using the softmax activation function to yield probabilities for each one of three classes\n",
        "    predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "    # Combining the base model with custom top layers\n",
        "    model = tensorflow.keras.Model(inputs=base_model.input,\n",
        "                                   outputs=predictions)\n",
        "    return model\n",
        "\n",
        "model = create_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAjxho43xNSs"
      },
      "source": [
        "#Setting up the optimizer, defining evaluation metrics, and compiling the model for training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCsoKbaPPJBs"
      },
      "outputs": [],
      "source": [
        "# Define the Adam optimizer with a learning rate of 0.001 and assign it to the variable OPT\n",
        "OPT = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "# Define a list of evaluation metrics to be used during model training and validation\n",
        "METRICS = [\n",
        "    tf.keras.metrics.CategoricalAccuracy(name='acc'),  # Computes accuracy for multi-class classification\n",
        "    tf.keras.metrics.AUC(name='auc'),                  # Computes area under the ROC curve for binary classification\n",
        "    tfa.metrics.F1Score(num_classes=3)                 # Computes F1 score for multi-class classification with 3 classes\n",
        "]\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=OPT, loss='categorical_crossentropy', metrics=METRICS)\n",
        "\n",
        "from contextlib import redirect_stdout\n",
        "with open('modelsummary_model.txt', 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        model.summary(show_trainable = True)\n",
        "\n",
        "# Display model summary\n",
        "model.summary(show_trainable = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aa4A1uh1DUjv"
      },
      "outputs": [],
      "source": [
        "font = ImageFont.truetype('arial.ttf', 12)\n",
        "visualkeras.layered_view(model, to_file='model_viskeras.png', legend=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBPKLXlhDZAX"
      },
      "outputs": [],
      "source": [
        "#Calling plot_model method and displaying a model structure summary\n",
        "plot_model(model, to_file='model_structure.png', show_shapes=True, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=100, show_layer_activations=True, show_trainable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8_1mSqXyXNy"
      },
      "source": [
        "#Defining callbacks\n",
        "Essential for monitoring the training process, preventing overfitting, and saving the best model weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGVacZ-BPMjt"
      },
      "outputs": [],
      "source": [
        "# Configure early stopping, halting training when the validation loss stops decreasing\n",
        "earlystopping = EarlyStopping(\n",
        "    monitor='val_loss',    # Monitor the validation loss during training\n",
        "    mode='min',            # Stop training when the monitored quantity (val_loss) stops decreasing\n",
        "    patience=7,            # Number of epochs with no improvement after which training will be stopped\n",
        "    verbose=1,             # Print messages about early stopping progress during training\n",
        "    restore_best_weights = True\n",
        ")\n",
        "\n",
        "# Define the filepath where the best model weights will be saved during training\n",
        "filepath = './model_best_weights.keras'\n",
        "\n",
        "# Configure model checkpointing to save the best model weights based on validation loss\n",
        "checkpoint = ModelCheckpoint(\n",
        "    filepath,              # Filepath to save the best model weights\n",
        "    monitor='val_loss',    # Monitor the validation loss during training\n",
        "    mode='min',            # Save the weights when the monitored quantity (val_loss) achieves its minimum value\n",
        "    save_best_only=True,   # Save only the best model weights (with the lowest validation loss)\n",
        "    verbose=1              # Print messages about model checkpointing progress during training\n",
        ")\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                              factor=0.2,\n",
        "                              patience=3,\n",
        "                              min_lr=1e-6)\n",
        "\n",
        "# Create a list of callbacks to be used during model training, including early stopping and model checkpointing\n",
        "callback_list = [earlystopping, checkpoint, reduce_lr]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03F1VfoM2yXK"
      },
      "source": [
        "#Training and Validating a Model\n",
        "\n",
        "Fitting the training data to a machine learning model and validating its performance using separate validation data. The code employs callbacks specified in `callback_list` to monitor and control the training process. The `epochs` parameter determines the number of times the entire training dataset is passed forward and backward through the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lY8qzwvlikFe"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_images,\n",
        "    validation_data=val_images,\n",
        "    callbacks=callback_list,\n",
        "    epochs=EPOCHS,\n",
        "    class_weight=class_weights  # Add class weights to handle class imbalance\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efZqzBcE3QZW"
      },
      "source": [
        "#Monitoring Model Performance with Accuracy and Loss Plots\n",
        "\n",
        "Generating four plots to monitor the performance of a machine learning model during training and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P73x6k9kP3nH"
      },
      "outputs": [],
      "source": [
        "# Create a figure and axes for the accuracy plot\n",
        "plt.figure(figsize=(8, 6))  # Set the size of the figure\n",
        "plt.grid(True, linestyle=':', linewidth=0.5, color='gray')  # Add a delicate grid in the background\n",
        "plt.plot(history.history['acc'], linestyle='-', label='Training')  # Plot training accuracy without markers\n",
        "plt.plot(history.history['val_acc'], linestyle='--', label='Validation')  # Plot validation accuracy without markers\n",
        "plt.title('Model Accuracy')  # Set title for the plot\n",
        "plt.ylabel('Accuracy')  # Label y-axis\n",
        "plt.xlabel('Epoch')  # Label x-axis\n",
        "plt.xticks(range(0, len(history.history['acc'])+1, max(1, len(history.history['acc'])//10)))  # Dynamic x-axis based on the number of epochs\n",
        "\n",
        "# Find and mark the best epoch for validation accuracy\n",
        "best_epoch_val_acc = history.history['val_acc'].index(max(history.history['val_acc']))\n",
        "plt.scatter(best_epoch_val_acc, max(history.history['val_acc']), color='green', label=f'Best Epoch ({best_epoch_val_acc + 1})')\n",
        "\n",
        "plt.legend(loc='lower right')  # Add legend to indicate which line represents train and validation\n",
        "plt.tight_layout()  # Adjust layout to prevent overlapping of elements\n",
        "plt.savefig('acc.png')\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Create a figure and axes for the loss plot\n",
        "plt.figure(figsize=(8, 6))  # Set the size of the figure\n",
        "plt.grid(True, linestyle=':', linewidth=0.5, color='gray')  # Add a delicate grid in the background\n",
        "plt.plot(history.history['loss'], linestyle='-', label='Training')  # Plot training loss without markers\n",
        "plt.plot(history.history['val_loss'], linestyle='--', label='Validation')  # Plot validation loss without markers\n",
        "plt.title('Model Loss')  # Set title for the plot\n",
        "plt.ylabel('Loss')  # Label y-axis\n",
        "plt.xlabel('Epoch')  # Label x-axis\n",
        "plt.xticks(range(0, len(history.history['loss'])+1, max(1, len(history.history['loss'])//10)))  # Dynamic x-axis based on the number of epochs\n",
        "\n",
        "# Find and mark the best epoch for validation loss\n",
        "best_epoch_val_loss = history.history['val_loss'].index(min(history.history['val_loss']))\n",
        "plt.scatter(best_epoch_val_loss, min(history.history['val_loss']), color='green', label=f'Best Epoch ({best_epoch_val_loss + 1})')\n",
        "\n",
        "plt.legend(loc='upper right')  # Add legend to indicate which line represents train and validation\n",
        "plt.tight_layout()  # Adjust layout to prevent overlapping of elements\n",
        "plt.savefig('loss.png')\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Create a figure and axes for the AUC plot\n",
        "plt.figure(figsize=(8, 6))  # Set the size of the figure\n",
        "plt.grid(True, linestyle=':', linewidth=0.5, color='gray')  # Add a delicate grid in the background\n",
        "plt.plot(history.history['auc'], linestyle='-', label='Training')  # Plot training AUC without markers\n",
        "plt.plot(history.history['val_auc'], linestyle='--', label='Validation')  # Plot validation AUC without markers\n",
        "plt.title('Model AUC')  # Set title for the plot\n",
        "plt.ylabel('AUC')  # Label y-axis\n",
        "plt.xlabel('Epoch')  # Label x-axis\n",
        "plt.xticks(range(0, len(history.history['auc'])+1, max(1, len(history.history['auc'])//10)))  # Dynamic x-axis based on the number of epochs\n",
        "\n",
        "# Find and mark the best epoch for validation AUC\n",
        "best_epoch_val_auc = history.history['val_auc'].index(max(history.history['val_auc']))\n",
        "plt.scatter(best_epoch_val_auc, max(history.history['val_auc']), color='green', label=f'Best Epoch ({best_epoch_val_auc + 1})')\n",
        "\n",
        "plt.legend(loc='lower right')  # Add legend to indicate which line represents train and validation\n",
        "plt.tight_layout()  # Adjust layout to prevent overlapping of elements\n",
        "plt.savefig('auc.png')\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Get F1 scores from the history\n",
        "f1_scores_train = history.history['f1_score']\n",
        "f1_scores_val = history.history['val_f1_score']\n",
        "\n",
        "# Create a figure and axes for the F1 score plot\n",
        "plt.figure(figsize=(8, 6))  # Set the size of the figure\n",
        "plt.grid(True, linestyle=':', linewidth=0.5, color='gray')  # Add a delicate grid in the background\n",
        "\n",
        "# Plot training F1 score with a solid line\n",
        "f1_scores_train_avg = np.mean(f1_scores_train, axis=1)\n",
        "plt.plot(f1_scores_train_avg, linestyle='-', label='Training')\n",
        "\n",
        "# Plot validation F1 score with a dashed line\n",
        "f1_scores_val_avg = np.mean(f1_scores_val, axis=1)\n",
        "plt.plot(f1_scores_val_avg, linestyle='--', label='Validation')\n",
        "\n",
        "# Find and mark the best epoch for validation F1 score\n",
        "best_epoch_val_f1 = np.argmax(f1_scores_val_avg)\n",
        "plt.scatter(best_epoch_val_f1, f1_scores_val_avg[best_epoch_val_f1], color='green', zorder=5, label=f'Best Epoch ({best_epoch_val_f1 + 1})')\n",
        "\n",
        "plt.title('Model F1 Score')  # Set title for the plot\n",
        "plt.ylabel('F1 Score')  # Label y-axis\n",
        "plt.xlabel('Epoch')  # Label x-axis\n",
        "plt.xticks(range(0, len(f1_scores_train_avg) + 1, max(1, len(f1_scores_train_avg) // 10)))  # Dynamic x-axis based on the number of epochs\n",
        "\n",
        "plt.legend(loc='lower right')  # Add legend to indicate which line represents train and validation\n",
        "plt.tight_layout()  # Adjust layout to prevent overlapping of elements\n",
        "plt.savefig('f1_score.png')\n",
        "plt.show()  # Display the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNX6H4Yk6Bcb"
      },
      "source": [
        "#Evaluating Model Performance and Generating Classification Report\n",
        "\n",
        "The performance of a machine learning model is evaluated using the test dataset.\n",
        "\n",
        "First, the `evaluate` method is called on the model with the test images, providing evaluation scores. The accuracy of the model on the test data is then printed.\n",
        "\n",
        "Next, the model predicts labels for the test images using the `predict` method. The predicted labels are then processed using a `roundoff` function, which rounds off each predicted label array by setting non-maximum values to 0 and the maximum value to 1.\n",
        "\n",
        "After rounding off the predicted labels, the index of the maximum value along axis 1 is extracted to get the final predicted labels.\n",
        "\n",
        "Finally, a classification report is generated using the `classification_report` function from scikit-learn, comparing the predicted labels against the true labels in the test dataset. The classification report provides metrics such as precision, recall, and F1-score for each class, aiding in the evaluation of the model's performance across different classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPVNt7Nq1IxX"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/model_best_weights.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5Z6yNPvQLlE"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model_path = '/content/model_best_weights.keras'\n",
        "\n",
        "# Load the model\n",
        "loaded_model = load_model(model_path)\n",
        "\n",
        "test_scores = loaded_model.evaluate(test_images)  # Evaluate the model on test data\n",
        "print(\"Testing Accuracy: %.2f%%\"%(test_scores[1] * 100))  # Print the testing accuracy\n",
        "\n",
        "pred_labels = loaded_model.predict(test_images)  # Predict labels for test images\n",
        "\n",
        "def roundoff(arr):\n",
        "    \"\"\"To round off according to the argmax of each predicted label array.\"\"\"\n",
        "    arr[np.argwhere(arr != arr.max())] = 0  # Set non-maximum values to 0\n",
        "    arr[np.argwhere(arr == arr.max())] = 1  # Set maximum value to 1\n",
        "    return arr\n",
        "\n",
        "for labels in pred_labels:\n",
        "    labels = roundoff(labels)  # Round off the predicted labels\n",
        "\n",
        "pred = np.argmax(pred_labels,axis=1)  # Get the index of the maximum value along the axis 1\n",
        "\n",
        "print(classification_report(test_images.classes,pred,target_names=CLASSES))  # Print classification report\n",
        "\n",
        "\n",
        "import sys\n",
        "# Save the original standard output\n",
        "original_stdout = sys.stdout\n",
        "\n",
        "# Open a file in write mode to save the information\n",
        "with open('loaded_model_evaluation_info.txt', 'w') as f:\n",
        "    # Redirect the standard output to the file\n",
        "    sys.stdout = f\n",
        "\n",
        "    # Evaluate the model on test data\n",
        "    test_scores = loaded_model.evaluate(test_images)\n",
        "    print(\"Testing Accuracy: %.2f%%\"%(test_scores[1] * 100))  # Print the testing accuracy\n",
        "\n",
        "    # Predict labels for test images\n",
        "    pred_labels = loaded_model.predict(test_images)\n",
        "\n",
        "    def roundoff(arr):\n",
        "        \"\"\"To round off according to the argmax of each predicted label array.\"\"\"\n",
        "        arr[np.argwhere(arr != arr.max())] = 0  # Set non-maximum values to 0\n",
        "        arr[np.argwhere(arr == arr.max())] = 1  # Set maximum value to 1\n",
        "        return arr\n",
        "\n",
        "    for labels in pred_labels:\n",
        "        labels = roundoff(labels)  # Round off the predicted labels\n",
        "\n",
        "    pred = np.argmax(pred_labels,axis=1)  # Get the index of the maximum value along the axis 1\n",
        "\n",
        "    # Print classification report to the file\n",
        "    print(classification_report(test_images.classes, pred, target_names=CLASSES))\n",
        "\n",
        "# Reset the standard output to the original value\n",
        "sys.stdout = original_stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eqx7Mw4Q6c6Q"
      },
      "source": [
        "#Visualizing Classification Performance with Confusion Matrix\n",
        "\n",
        "First, true and predicted labels are extracted from the test dataset and the model predictions, respectively. Then, the confusion matrix is computed using the `confusion_matrix` function from scikit-learn, which provides a summary of the model's predictions versus the actual labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-BGI7SNQTqv"
      },
      "outputs": [],
      "source": [
        "# Plot the confusion matrix to understand the classification in detail\n",
        "test_ls, pred_ls = test_images.classes, pred  # Get true and predicted labels\n",
        "conf_arr = confusion_matrix(test_ls, pred_ls)  # Compute confusion matrix\n",
        "\n",
        "# Compute percentages\n",
        "conf_arr_percent = conf_arr.astype('float') / conf_arr.sum(axis=1)[:, np.newaxis] * 100\n",
        "\n",
        "plt.figure(figsize=(8, 6), dpi=100)  # Set figure size and dpi\n",
        "plt.subplots_adjust(left=0.15, right=0.9, top=0.9, bottom=0.15)  # Adjust padding\n",
        "\n",
        "# Plot cm with annotations\n",
        "ax = sns.heatmap(conf_arr_percent, cmap='Blues', annot=True, fmt='.1f', cbar=True, xticklabels=CLASSES, yticklabels=CLASSES)\n",
        "\n",
        "for t in ax.texts:\n",
        "    t.set_text(t.get_text() + ' %')\n",
        "\n",
        "plt.title('Alzheimer\\'s Disease Diagnosis')  # Set title for the plot\n",
        "plt.xlabel('Predicted Labels')  # Set label for x-axis\n",
        "plt.ylabel('True Labels')  # Set label for y-axis\n",
        "plt.savefig('confusion_matrix_loaded_model.png', bbox_inches='tight')  # Save the plot with tight bounding box\n",
        "plt.show()  # Display the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqkkPJAR9yMj"
      },
      "source": [
        "#Analyzing Model Performance with ROC Curve and Thresholds\n",
        "\n",
        "Analysis of model performance using Receiver Operating Characteristic (ROC) curves by incorporating different classification thresholds.\n",
        "\n",
        "After converting true labels into a binary format, ROC curves and areas under the curve (AUC) are computed for each class. Additionally, the true positive rate (TPR) and false positive rate (FPR) are calculated for various classification thresholds.\n",
        "\n",
        "The micro-average ROC curve and AUC are derived by aggregating TPR and FPR across all classes, while the macro-average ROC curve and AUC represent the average performance across all classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asapBegFKywx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import roc_curve, auc, classification_report\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_scores = loaded_model.evaluate(test_images)\n",
        "print(\"Testing Accuracy: %.2f%%\" % (test_scores[1] * 100))\n",
        "\n",
        "# Predict probabilities for test images\n",
        "pred_labels = loaded_model.predict(test_images)\n",
        "\n",
        "# Convert labels to binary format where each class is represented by a binary vector\n",
        "y_true = label_binarize(test_images.classes, classes=range(len(CLASSES)))  # CLASSES is the list of class names\n",
        "n_classes = y_true.shape[1]\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "fpr = dict()  # False Positive Rate\n",
        "tpr = dict()  # True Positive Rate\n",
        "roc_auc = dict()  # Area under ROC curve\n",
        "\n",
        "# Thresholds for which to compute ROC\n",
        "thresholds = np.linspace(0, 1, 100)  # 100 thresholds between 0 and 1\n",
        "tpr_values = np.zeros((n_classes, len(thresholds)))\n",
        "fpr_values = np.zeros((n_classes, len(thresholds)))\n",
        "\n",
        "# Iterate over each class to compute ROC curve and area\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_true[:, i], pred_labels[:, i])  # Compute FPR, TPR, thresholds\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])  # Compute area under ROC curve\n",
        "\n",
        "    # Compute TPR and FPR for different thresholds\n",
        "    for j, threshold in enumerate(thresholds):\n",
        "        tp = np.sum((pred_labels[:, i] >= threshold) & (y_true[:, i] == 1))\n",
        "        fp = np.sum((pred_labels[:, i] >= threshold) & (y_true[:, i] == 0))\n",
        "        tn = np.sum((pred_labels[:, i] < threshold) & (y_true[:, i] == 0))\n",
        "        fn = np.sum((pred_labels[:, i] < threshold) & (y_true[:, i] == 1))\n",
        "\n",
        "        tpr_values[i, j] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        fpr_values[i, j] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))  # Obtain all unique FPR values\n",
        "mean_tpr = np.zeros_like(all_fpr)  # Initialize mean TPR\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])  # Interpolate TPR values\n",
        "mean_tpr /= n_classes  # Calculate mean TPR\n",
        "fpr[\"micro\"] = all_fpr\n",
        "tpr[\"micro\"] = mean_tpr\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])  # Compute area under micro-average ROC curve\n",
        "\n",
        "# Compute macro-average ROC curve and ROC area\n",
        "roc_auc_macro = auc(fpr[\"micro\"], tpr[\"micro\"])  # Compute area under macro-average ROC curve\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "lw = 2  # Adjust line width if needed\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "colors = ['aqua', 'darkorange', 'cornflowerblue']  # Adjust colors if needed\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.step(fpr_values[i], tpr_values[i], color=color, lw=lw, where='post',\n",
        "             label='ROC curve of class {0} (area = {1:0.3f})'.format(CLASSES[i], roc_auc[i]))\n",
        "\n",
        "# Plot micro-average ROC curve\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='deeppink', linestyle=':', linewidth=4,\n",
        "         label='Micro-average ROC curve (area = {0:0.3f})'.format(roc_auc[\"micro\"]))\n",
        "\n",
        "# Plot macro-average ROC curve\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"], color='navy', linestyle='--', linewidth=4,\n",
        "         label='Macro-average ROC curve (area = {0:0.3f})'.format(roc_auc_macro))\n",
        "\n",
        "# Plot points for different thresholds\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    for j, threshold in enumerate(thresholds[::10]):\n",
        "        plt.scatter(fpr_values[i, j*10], tpr_values[i, j*10], color=color, s=10)\n",
        "\n",
        "# Add random guess line\n",
        "plt.plot([0, 1], [0, 1], color='gray', lw=lw, linestyle='--')\n",
        "\n",
        "# Set plot properties\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve with Thresholds')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True, linestyle=':', linewidth=0.5, color='gray')  # Add a delicate grid in the background\n",
        "# Save the figure as a .png file\n",
        "plt.savefig('roc_curve_loaded_model_new.png')\n",
        "plt.show()\n",
        "\n",
        "# Print classification report\n",
        "pred = np.argmax(pred_labels, axis=1)  # Get the index of the maximum value along the axis 1\n",
        "print(classification_report(test_images.classes, pred, target_names=CLASSES))\n",
        "\n",
        "# # Save the evaluation and classification report to a file\n",
        "import sys\n",
        "\n",
        "# # Save the original standard output\n",
        "original_stdout = sys.stdout\n",
        "\n",
        "# # Open a file in write mode to save the information\n",
        "with open('loaded_model_evaluation_info.txt', 'w') as f:\n",
        "  # Redirect the standard output to the file\n",
        "  sys.stdout = f\n",
        "  # Evaluate the model on test data\n",
        "  test_scores = loaded_model.evaluate(test_images)\n",
        "  print(\"Testing Accuracy: %.2f%%\" % (test_scores[1] * 100))  # Print the testing accuracy\n",
        "\n",
        "  # Print classification report to the file\n",
        "  print(classification_report(test_images.classes, pred, target_names=CLASSES))\n",
        "\n",
        "  # Reset the standard output to the original value\n",
        "  sys.stdout = original_stdout"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Misclassifications for each class"
      ],
      "metadata": {
        "id": "9AD86E2H_uyB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGgTZ6XJJHT7"
      },
      "outputs": [],
      "source": [
        "# Initialize misclassifications for each class\n",
        "misclassifications = np.zeros(len(CLASSES), dtype=int)\n",
        "\n",
        "# Initialize total instances for each class\n",
        "total_instances = np.zeros(len(CLASSES), dtype=int)\n",
        "\n",
        "# Iterate over each instance in the test set\n",
        "for true_label, pred_label in zip(test_ls, pred_ls):\n",
        "    # Increment total instances count for the true label's class\n",
        "    total_instances[true_label] += 1\n",
        "    # If the true label and predicted label don't match, increment misclassification count for the true label's class\n",
        "    if true_label != pred_label:\n",
        "        misclassifications[true_label] += 1\n",
        "\n",
        "# Calculate misclassifications as percentages\n",
        "misclassifications_percentage = (misclassifications / total_instances) * 100\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))  # Set figure size\n",
        "plt.subplots_adjust(left=0.15, right=0.9, top=0.9, bottom=0.15)  # Adjust padding\n",
        "\n",
        "# Plot the bar chart\n",
        "bars = plt.bar(CLASSES, misclassifications_percentage, color='skyblue')\n",
        "plt.title('Misclassifications on Test Set by Class')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Misclassifications Percentage')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle=':', linewidth=0.5, color='gray')\n",
        "\n",
        "# Annotate each bar with its percentage value\n",
        "for bar, percentage in zip(bars, misclassifications_percentage):\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width() / 2, height, f'{percentage:.2f}%', ha='center', va='bottom')\n",
        "\n",
        "plt.savefig('missclass_loaded_model.png', bbox_inches='tight')  # Save the plot with tight bounding box\n",
        "plt.show()  # Display the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x92o3O4BGfkK"
      },
      "source": [
        "#Printing Additional Classification Metrics\n",
        "\n",
        "1. **Balanced Accuracy Score (BAS)** - a metric that provides a balanced measure of classification performance, particularly useful for imbalanced datasets. It considers both the sensitivity (true positive rate) and specificity (true negative rate) of the model's predictions.\n",
        "\n",
        "2. **Matthew's Correlation Coefficient (MCC)** - a metric that measures the quality of binary classifications, taking into account both true and false positives and negatives. It ranges from -1 to 1, where 1 indicates perfect predictions, 0 indicates random predictions, and -1 indicates complete disagreement between predictions and true labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmpD3TmEQiUq"
      },
      "outputs": [],
      "source": [
        "# Open a text file in write mode\n",
        "with open('classification_metrics_loaded_model.txt', 'w') as f:\n",
        "    # Write Balanced Accuracy Score (BAS) to the file\n",
        "    f.write(\"Balanced Accuracy Score: {} %\\n\".format(round(BAS(test_ls, pred_ls) * 100, 2)))\n",
        "\n",
        "    # Write Matthew's Correlation Coefficient (MCC) to the file\n",
        "    f.write(\"Matthew's Correlation Coefficient: {} %\\n\".format(round(MCC(test_ls, pred_ls) * 100, 2)))\n",
        "\n",
        "# Print a message indicating that the metrics have been saved\n",
        "print(\"Classification metrics have been saved to 'classification_metrics_loaded_model.txt'.\")\n",
        "\n",
        "# Printing some other classification metrics\n",
        "\n",
        "# Compute BAS and format the output as a percentage rounded to two decimal places.\n",
        "print(\"Balanced Accuracy Score: {} %\".format(round(BAS(test_ls, pred_ls) * 100, 2)))\n",
        "\n",
        "# Compute MCC and format the output as a percentage rounded to two decimal places.\n",
        "print(\"Matthew's Correlation Coefficient: {} %\".format(round(MCC(test_ls, pred_ls) * 100, 2)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}